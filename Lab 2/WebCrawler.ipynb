{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install beautifulsoup4\n",
    "!pip3 install lxml\n",
    "!pip3 install html5lib\n",
    "!pip3 install grequests\n",
    "!pip3 install gevent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv function definitions:\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "def saveSetToFile(my_set, csv_file_name):\n",
    "    for url in my_set:\n",
    "        appendRowToCsvFile(url, csv_file_name)\n",
    "        \n",
    "        \n",
    "def appendRowToCsvFile(csvRow, csv_file_name):\n",
    "    with open(csv_file_name, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([csvRow])\n",
    "\n",
    "        \n",
    "def loadCsvToSet(my_set, csv_file_name):\n",
    "    mycsv = csv.reader(open(csv_file_name))\n",
    "    for row in mycsv:\n",
    "        url = row[0]\n",
    "        my_set.add(url)\n",
    "\n",
    "        \n",
    "#saveSetToFile(crawled_links, csv_file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-3b8a240f1f04>, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-3b8a240f1f04>\"\u001b[0;36m, line \u001b[0;32m45\u001b[0m\n\u001b[0;31m    print(\"An exception ocurred : %s\" % str(e)) print(url)\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import urllib.robotparser as robotparser\n",
    "#import grequests\n",
    "\n",
    "\n",
    "#init:\n",
    "#initial_url_address = 'https://machinelearningmastery.com/compare-performance-machine-learning-algorithms-weka/'\n",
    "#initial_url_address = 'http://guidesarchive.ign.com/guides/10798/secrets.html'\n",
    "initial_url_address = 'https://es.wikipedia.org/wiki/Resident_Evil_2' \n",
    "current_batch = [initial_url_address] #initialize crawl batch\n",
    "crawled_links = {initial_url_address} #hold the overall url links repo\n",
    "max_depth = 3 #how deep is the breadth-first approach\n",
    "csv_file_name = \"crawlResultsCsv.csv\" #where you want the file to be saved to. At the moment, csv is in the same path\n",
    "\n",
    "\n",
    "print(\"starting the crawler...\")\n",
    "print(\"remember to do the robot thing!!\")\n",
    "\n",
    "\n",
    "#test async\n",
    "#def print_url(r, *args, **kwargs):\n",
    "#    print(r.url)\n",
    "\n",
    "#rs = (grequests.get(u) for u in current_batch)\n",
    "#grequests.map(rs)\n",
    "#grequests.get(u, callback=print_url)\n",
    "        \n",
    "\n",
    "#TODO: \n",
    "# 1. Confirm that it agrees to be checked (robots.txt, crawling frequency) => NOT SURE\n",
    "# 2. Make asyncronous\n",
    "\n",
    "\n",
    "\n",
    "def canFetchUrl(url):\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    can_fetch = False\n",
    "    try:\n",
    "        rp.set_url(url) #robotparser get address\n",
    "        rp.read() #robotparser read  \n",
    "        can_fetch = rp.can_fetch(\"*\",url) # true if we can fetch the url according to robot.txt\n",
    "    except BaseException as e:\n",
    "        print(\"An exception ocurred : %s\" % str(e)) \n",
    "        print(url)\n",
    "        print(\"\")\n",
    "        print(\". . .\")     \n",
    "    return can_fetch\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def getLinksFromPage(url):\n",
    "    page_links = [] #links in this page\n",
    "        \n",
    "    resp = requests.get(url)\n",
    "    html = BeautifulSoup(resp.text, 'html.parser') #get html\n",
    "    for a in html.find_all('a'):\n",
    "        link = a.get('href')\n",
    "        if isinstance(link, str) and link.startswith('http'):\n",
    "            link = link.lower()\n",
    "            appendRowToCsvFile(link, csv_file_name)\n",
    "            page_links.append(link) #extract href url\n",
    "    return page_links\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "# main loop\n",
    "for i in range(max_depth):\n",
    "    print(\"\")\n",
    "    print(\". . .\")\n",
    "    print(\"step: %s\" % (i+1))\n",
    "    print(\"batch size: %s\" % len(current_batch))\n",
    "    print(\"crawled unique links: %s\" % len(crawled_links))\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "\n",
    "    next_batch = []\n",
    "    \n",
    "    for url in current_batch:\n",
    "        page_links = getLinksFromPage(url)\n",
    "        for link in page_links:          \n",
    "            if link not in crawled_links:\n",
    "                if canFetchUrl(link):\n",
    "                    #Here we only save the allowed links\n",
    "                    print(\"allowed to fetch : %s\" % link)\n",
    "                    crawled_links.add(link)\n",
    "                    appendRowToCsvFile(link, csv_file_name)\n",
    "                    next_batch.append(link) #since it was a new link, we will crawl it in the next batch                    \n",
    "                else: print(\"not allowed to fetch : %s\" % link)\n",
    "    current_batch = next_batch\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"\")\n",
    "print(\". . .\")\n",
    "print(\"FINISH!\")\n",
    "print(\"unique links size: %s\" % len(crawled_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434\n"
     ]
    }
   ],
   "source": [
    "# Optional, for loading the results CSV:\n",
    "test_set={initial_url_address}\n",
    "loadCsvToSet(test_set, csv_file_name)\n",
    "\n",
    "print(len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.robotparser as robotparser\n",
    "rp = robotparser.RobotFileParser()\n",
    "\n",
    "def canFetchUrl(url):\n",
    "    rp.set_url(url) #robotparser get address\n",
    "    rp.read() #robotparser read\n",
    "    #TODO: if false, do not fetch\n",
    "    #TODO: not sure if this works like this.\n",
    "    can_fetch = rp.can_fetch(\"*\",url) # true if we can fetch the url according to robot.txt\n",
    "    return can_fetch\n",
    "\n",
    "try:\n",
    "    print(canFetchUrl('http://guidesarchive.ign.com/guides/10798/rankings.html'))\n",
    "except BaseException as e:\n",
    "        print(\"An exception ocurred : %s\" % str(e))\n",
    "try:\n",
    "    print(canFetchUrl('http://guidesarchive.ign.com/guides/10798/rankings.html'))\n",
    "except BaseException as e:\n",
    "        print(\"An exception ocurred : %s\" % str(e))\n",
    "        \n",
    "print(canFetchUrl('http://www.capcom.co.jp/newproducts/consumer/bio2/zapping.html'))\n",
    "print(canFetchUrl('http://www.vgchartz.com/gamedb/?name=resident+evil+2&publisher=&platform=&genre=&minsales=0&results=200'))\n",
    "print(canFetchUrl('http://www.webcitation.org/5xtbyewcd'))\n",
    "\n",
    "try:\n",
    "    print(canFetchUrl('http://guidesarchive.ign.com/guides/10798/rankings.html'))\n",
    "except BaseException as e:\n",
    "        print(\"An exception ocurred : %s\" % str(e))\n",
    "print(canFetchUrl('http://www.gamespot.com/reviews/resident-evil-2-review/1900-2549080/'))\n",
    "print(canFetchUrl('http://cube.gamespy.com/gamecube/resident-evil-2/5580p1.html'))\n",
    "print(canFetchUrl('http://www.capcom.co.jp/ir/english/business/million.html'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ww 44\n"
     ]
    }
   ],
   "source": [
    "v=\"ss\"\n",
    "print(\"ww %s\" % 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
