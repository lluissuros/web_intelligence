{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install beautifulsoup4\n",
    "!pip3 install lxml\n",
    "!pip3 install html5lib\n",
    "!pip3 install grequests\n",
    "!pip3 install gevent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv function definitions:\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "def saveSetToFile(my_set, csv_file_name):\n",
    "    for url in my_set:\n",
    "        appendRowToCsvFile(url, csv_file_name)\n",
    "        \n",
    "        \n",
    "def appendRowToCsvFile(csvRow, csv_file_name):\n",
    "    with open(csv_file_name, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([csvRow])\n",
    "\n",
    "        \n",
    "def loadCsvToSet(my_set, csv_file_name):\n",
    "    mycsv = csv.reader(open(csv_file_name))\n",
    "    for row in mycsv:\n",
    "        url = row[0]\n",
    "        my_set.add(url)\n",
    "\n",
    "        \n",
    "#saveSetToFile(crawled_links, csv_file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url helper functions\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.robotparser as robotparser\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "def does_url_exist(url):\n",
    "    try: \n",
    "        r = requests.head(url)\n",
    "        if r.status_code < 400:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        #print(e)\n",
    "        return False\n",
    "\n",
    "    \n",
    "def getUrlBase(url):\n",
    "    url_parsed = urlparse(url)\n",
    "    return (url_parsed.scheme + '://' + url_parsed.netloc)\n",
    "    \n",
    "        \n",
    "def hasRobotText(url):\n",
    "    return does_url_exist(urljoin(getUrlBase(url), 'robots.txt'))\n",
    "                          \n",
    "\n",
    "def canFetchUrl(url):\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    can_fetch = False\n",
    "    robotUrl = urljoin(getUrlBase(url), 'robots.txt')          \n",
    "    if hasRobotText(robotUrl):\n",
    "        try:\n",
    "            rp.set_url(robotUrl) #robotparser get address\n",
    "            rp.read() #robotparser read  \n",
    "            can_fetch = rp.can_fetch(\"*\",url) # true if we can fetch the url according to robot.txt\n",
    "        except BaseException as e:\n",
    "            print(\"An exception ocurred : %s\" % str(e)) \n",
    "            #print(url)\n",
    "            #print(\"\")\n",
    "            #print(\". . .\") \n",
    "    else: can_fetch = True #no robot means everything allowed\n",
    "    return can_fetch\n",
    "\n",
    "\n",
    "def getLinksFromPage(url):\n",
    "    page_links = [] #links in this page\n",
    "        \n",
    "    resp = requests.get(url)\n",
    "    html = BeautifulSoup(resp.text, 'html.parser') #get html\n",
    "    for a in html.find_all('a'):\n",
    "        link = a.get('href')\n",
    "        if isinstance(link, str) and link.startswith('http'):\n",
    "            page_links.append(link.lower()) #extract href url\n",
    "    return page_links\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the crawler...\n",
      "saving results on crawlResults_1549563162.csv\n",
      "\n",
      ". . .\n",
      "step: 1\n",
      "pages to crawl in this step: 1\n",
      "crawled unique links so far: 1\n",
      ". . .\n",
      "\n",
      "\n",
      "fetching page 1, it might take a while : https://en.wikipedia.org/wiki/Web_intelligence\n",
      "allowed new links found in page: 15\n",
      "\n",
      ". . .\n",
      "step: 2\n",
      "pages to crawl in this step: 15\n",
      "crawled unique links so far: 16\n",
      ". . .\n",
      "\n",
      "\n",
      "fetching page 2, it might take a while : http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=884768\n",
      "allowed new links found in page: 15\n",
      "\n",
      "fetching page 3, it might take a while : http://www.iospress.nl/journal/web-intelligence\n",
      "allowed new links found in page: 38\n",
      "\n",
      ". . .\n",
      "step: 3\n",
      "pages to crawl in this step: 0\n",
      "crawled unique links so far: 69\n",
      ". . .\n",
      "\n",
      "\n",
      ". . .\n",
      "step: 4\n",
      "pages to crawl in this step: 0\n",
      "crawled unique links so far: 69\n",
      ". . .\n",
      "\n",
      "\n",
      ". . .\n",
      "step: 5\n",
      "pages to crawl in this step: 0\n",
      "crawled unique links so far: 69\n",
      ". . .\n",
      "\n",
      "\n",
      ". . .\n",
      "FINISH!\n",
      "unique links size: 69\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#import grequests\n",
    "\n",
    "\n",
    "#init:\n",
    "initial_url_address = 'https://en.wikipedia.org/wiki/Web_intelligence' \n",
    "current_batch = [initial_url_address] #initialize crawl batch\n",
    "crawled_links = {initial_url_address} #hold the overall url links repo\n",
    "max_crawl_depth = 5 #how deep is the breadth-first approach\n",
    "max_crawl_pages = 100 #limit of web pages to crawl\n",
    "\n",
    "csv_file_name = \"crawlResults_\" + str(round(time.time())) + \".csv\" #csv unique name\n",
    "\n",
    "print(\"starting the crawler...\")\n",
    "print(\"saving results on %s\" %csv_file_name)\n",
    "        \n",
    "\n",
    "#TODO: \n",
    "# 1. Cache robots.txt, otherwise it is constatntly requesting!\n",
    "# 2. Make asyncronous\n",
    "  \n",
    "    \n",
    "crawled_pages = 0\n",
    "    \n",
    "# main loop\n",
    "for i in range(max_crawl_depth):\n",
    "    print(\"\")\n",
    "    print(\". . .\")\n",
    "    print(\"step: %s\" % (i+1))\n",
    "    print(\"pages to crawl in this step: %s\" % len(current_batch))\n",
    "    print(\"crawled unique links so far: %s\" % len(crawled_links))\n",
    "    print(\". . .\")\n",
    "    print(\"\")\n",
    "\n",
    "    next_batch = []\n",
    "    \n",
    "    for url in current_batch:\n",
    "        page_links = getLinksFromPage(url)\n",
    "        crawled_pages +=1\n",
    "        \n",
    "        if (crawled_pages > max_crawl_pages):\n",
    "            current_batch = []\n",
    "            break\n",
    "        else:\n",
    "            print(\"\")\n",
    "            print(\"fetching page %s\" % crawled_pages + \", it might take a while : %s\" % url)\n",
    "            new_links = 0\n",
    "            for link in page_links:\n",
    "                if link not in crawled_links:\n",
    "                    if canFetchUrl(link):\n",
    "                        new_links +=1\n",
    "                        #Here we only save the allowed links for the next batch\n",
    "                        crawled_links.add(link)\n",
    "                        appendRowToCsvFile(link, csv_file_name)\n",
    "                        next_batch.append(link) #since it was a new link, we will crawl it in the next batch                    \n",
    "                    #else: print(\"not allowed to fetch : %s\" % link)\n",
    "            print(\"allowed new links found in page: %s\" % new_links)\n",
    "        current_batch = []\n",
    "        current_batch = next_batch\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"\")\n",
    "print(\". . .\")\n",
    "print(\"FINISH!\")\n",
    "print(\"unique links size: %s\" % len(crawled_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "# Optional, for loading the results CSV:\n",
    "test_set={initial_url_address}\n",
    "loadCsvToSet(test_set, csv_file_name)\n",
    "\n",
    "print(len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "..\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## test robots.txt\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "test1 = 'https://www.buzzfeed.com/buzzfeed/api/' #not allowed by robot.txt\n",
    "test2 = 'https://github.com/lluissuros?tab=overview&from=2019-02-01&to=2019-02-07' # not allowed?\n",
    "test3 = 'https://slate.com/culture/sports' #allowed\n",
    "test4 = 'http://www.capcom.co.jp/newproducts/consumer/value/' #no robots.txt, therefore allowed\n",
    "\n",
    "\n",
    "\n",
    "print(hasRobotText(test1))\n",
    "print(hasRobotText(test2))\n",
    "print(hasRobotText(test3))\n",
    "print(hasRobotText(test4))\n",
    "\n",
    "print(\"..\")\n",
    "\n",
    "print(canFetchUrl(test1))\n",
    "print(canFetchUrl(test2))\n",
    "print(canFetchUrl(test3))\n",
    "print(canFetchUrl(test4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1383"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crawled_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(csv_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'https://help.imdb.com/imdb?ref_=cons_ftr_imdb' not in crawled_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
