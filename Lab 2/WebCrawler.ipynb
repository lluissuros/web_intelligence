{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install beautifulsoup4\n",
    "!pip3 install lxml\n",
    "!pip3 install html5lib\n",
    "!pip3 install grequests\n",
    "!pip3 install gevent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv function definitions:\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "def saveSetToFile(my_set, csv_file_name):\n",
    "    for url in my_set:\n",
    "        appendRowToCsvFile(url, csv_file_name)\n",
    "        \n",
    "        \n",
    "def appendRowToCsvFile(csvRow, csv_file_name):\n",
    "    with open(csv_file_name, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([csvRow])\n",
    "\n",
    "        \n",
    "def loadCsvToSet(my_set, csv_file_name):\n",
    "    mycsv = csv.reader(open(csv_file_name))\n",
    "    for row in mycsv:\n",
    "        url = row[0]\n",
    "        my_set.add(url)\n",
    "\n",
    "        \n",
    "#saveSetToFile(crawled_links, csv_file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url helper functions\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.robotparser as robotparser\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "def does_url_exist(url):\n",
    "    try: \n",
    "        r = requests.head(url)\n",
    "        if r.status_code < 400:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        #print(e)\n",
    "        return False\n",
    "\n",
    "    \n",
    "def getUrlBase(url):\n",
    "    url_parsed = urlparse(url)\n",
    "    return (url_parsed.scheme + '://' + url_parsed.netloc)\n",
    "    \n",
    "        \n",
    "def hasRobotText(url):\n",
    "    return does_url_exist(urljoin(getUrlBase(url), 'robots.txt'))\n",
    "                          \n",
    "\n",
    "def canFetchUrl(url):\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    can_fetch = False\n",
    "    robotUrl = urljoin(getUrlBase(url), 'robots.txt')          \n",
    "    if hasRobotText(robotUrl):\n",
    "        try:\n",
    "            rp.set_url(robotUrl) #robotparser get address\n",
    "            rp.read() #robotparser read  \n",
    "            can_fetch = rp.can_fetch(\"*\",url) # true if we can fetch the url according to robot.txt\n",
    "        except BaseException as e:\n",
    "            print(\"An exception ocurred : %s\" % str(e)) \n",
    "            #print(url)\n",
    "            #print(\"\")\n",
    "            #print(\". . .\") \n",
    "    else: can_fetch = True #no robot means everything allowed\n",
    "    return can_fetch\n",
    "\n",
    "\n",
    "def getLinksFromPage(url):\n",
    "    page_links = [] #links in this page\n",
    "        \n",
    "    resp = requests.get(url)\n",
    "    html = BeautifulSoup(resp.text, 'html.parser') #get html\n",
    "    for a in html.find_all('a'):\n",
    "        link = a.get('href')\n",
    "        if isinstance(link, str) and link.startswith('http'):\n",
    "            link = link.lower()\n",
    "            appendRowToCsvFile(link, csv_file_name)\n",
    "            page_links.append(link) #extract href url\n",
    "    return page_links\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the crawler...\n",
      "saving results on crawlResultsCsv_1549551725.csv\n",
      "\n",
      ". . .\n",
      "step: 1\n",
      "batch size: 1\n",
      "crawled unique links: 1\n",
      "\n",
      "\n",
      "\n",
      "fetching page 0, it might take a while : https://es.wikipedia.org/wiki/Resident_Evil_2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#import grequests\n",
    "\n",
    "\n",
    "#init:\n",
    "#initial_url_address = 'https://machinelearningmastery.com/compare-performance-machine-learning-algorithms-weka/'\n",
    "#initial_url_address = 'http://guidesarchive.ign.com/guides/10798/secrets.html'\n",
    "initial_url_address = 'https://es.wikipedia.org/wiki/Resident_Evil_2' \n",
    "current_batch = [initial_url_address] #initialize crawl batch\n",
    "crawled_links = {initial_url_address} #hold the overall url links repo\n",
    "max_depth = 5 #how deep is the breadth-first approach\n",
    "max_crawl_pages = 5\n",
    "\n",
    "csv_file_name = \"crawlResultsCsv_\" + str(round(time.time())) + \".csv\" #csv unique name\n",
    "\n",
    "print(\"starting the crawler...\")\n",
    "print(\"saving results on %s\" %csv_file_name)\n",
    "        \n",
    "\n",
    "#TODO: \n",
    "# 2. Make asyncronous\n",
    "  \n",
    "    \n",
    "crawled_pages = 0\n",
    "    \n",
    "# main loop\n",
    "for i in range(max_depth):\n",
    "    print(\"\")\n",
    "    print(\". . .\")\n",
    "    print(\"step: %s\" % (i+1))\n",
    "    print(\"batch size: %s\" % len(current_batch))\n",
    "    print(\"crawled unique links: %s\" % len(crawled_links))\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "\n",
    "    next_batch = []\n",
    "    \n",
    "    for url in current_batch:\n",
    "        print(\"\")\n",
    "        print(\"fetching page %s\" % crawled_pages + \", it might take a while : %s\" % url)\n",
    "        print(\"\")\n",
    "        \n",
    "        page_links = getLinksFromPage(url)\n",
    "        if (crawled_pages >= max_crawl_pages):\n",
    "            break\n",
    "        else: \n",
    "            for link in page_links:\n",
    "                if link not in crawled_links:\n",
    "                    if canFetchUrl(link):\n",
    "                        #Here we only save the allowed links for the next batch\n",
    "                        crawled_links.add(link)\n",
    "                        appendRowToCsvFile(link, csv_file_name)\n",
    "                        next_batch.append(link) #since it was a new link, we will crawl it in the next batch                    \n",
    "                    #else: print(\"not allowed to fetch : %s\" % link)\n",
    "            crawled_pages =+1\n",
    "        current_batch = []\n",
    "        current_batch = next_batch\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"\")\n",
    "print(\". . .\")\n",
    "print(\"FINISH!\")\n",
    "print(\"unique links size: %s\" % len(crawled_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533\n"
     ]
    }
   ],
   "source": [
    "# Optional, for loading the results CSV:\n",
    "test_set={initial_url_address}\n",
    "loadCsvToSet(test_set, csv_file_name)\n",
    "\n",
    "print(len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "..\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## test robots.txt\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "test1 = 'https://www.buzzfeed.com/buzzfeed/api/' #not allowed by robot.txt\n",
    "test2 = 'https://github.com/lluissuros?tab=overview&from=2019-02-01&to=2019-02-07' # not allowed?\n",
    "test3 = 'https://slate.com/culture/sports' #allowed\n",
    "test4 = 'http://www.capcom.co.jp/newproducts/consumer/value/' #no robots.txt, therefore allowed\n",
    "\n",
    "\n",
    "print(hasRobotText(test1))\n",
    "print(hasRobotText(test2))\n",
    "print(hasRobotText(test3))\n",
    "print(hasRobotText(test4))\n",
    "\n",
    "print(\"..\")\n",
    "\n",
    "print(canFetchUrl(test1))\n",
    "print(canFetchUrl(test2))\n",
    "print(canFetchUrl(test3))\n",
    "print(canFetchUrl(test4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
