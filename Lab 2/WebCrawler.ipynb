{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install beautifulsoup4\n",
    "!pip3 install lxml\n",
    "!pip3 install html5lib\n",
    "!pip3 install grequests\n",
    "!pip3 install gevent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv function definitions:\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "def saveSetToFile(my_set, csv_file_name):\n",
    "    for url in my_set:\n",
    "        appendRowToCsvFile(url, csv_file_name)\n",
    "        \n",
    "        \n",
    "def appendRowToCsvFile(csvRow, csv_file_name):\n",
    "    with open(csv_file_name, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([csvRow])\n",
    "\n",
    "        \n",
    "def loadCsvToSet(my_set, csv_file_name):\n",
    "    mycsv = csv.reader(open(csv_file_name))\n",
    "    for row in mycsv:\n",
    "        url = row[0]\n",
    "        my_set.add(url)\n",
    "\n",
    "        \n",
    "#saveSetToFile(crawled_links, csv_file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url helper functions\n",
    "import requests\n",
    "import urllib.robotparser as robotparser\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "def does_url_exist(url):\n",
    "    try: \n",
    "        r = requests.head(url)\n",
    "        if r.status_code < 400:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "        return False\n",
    "\n",
    "    \n",
    "def getUrlBase(url):\n",
    "    url_parsed = urlparse(url)\n",
    "    return (url_parsed.scheme + '://' + url_parsed.netloc)\n",
    "    \n",
    "        \n",
    "def hasRobotText(url):\n",
    "    return does_url_exist(urljoin(getUrlBase(url), 'robots.txt'))\n",
    "                          \n",
    "\n",
    "def canFetchUrl(url):\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    can_fetch = False\n",
    "    robotUrl = urljoin(getUrlBase(url), 'robots.txt')          \n",
    "    if hasRobotText(robotUrl):\n",
    "        try:\n",
    "            rp.set_url(robotUrl) #robotparser get address\n",
    "            rp.read() #robotparser read  \n",
    "            can_fetch = rp.can_fetch(\"*\",url) # true if we can fetch the url according to robot.txt\n",
    "        except BaseException as e:\n",
    "            print(\"An exception ocurred : %s\" % str(e)) \n",
    "            print(url)\n",
    "            print(\"\")\n",
    "            print(\". . .\") \n",
    "    else: can_fetch = True #no robot means everything allowed\n",
    "    return can_fetch        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the crawler...\n",
      "remember to do the robot thing!!\n",
      "\n",
      ". . .\n",
      "step: 1\n",
      "batch size: 1\n",
      "crawled unique links: 1\n",
      "\n",
      "\n",
      "allowed to fetch : https://www.wikidata.org/wiki/q733992?uselang=es#p348\n",
      "allowed to fetch : https://www.wikidata.org/wiki/q733992?uselang=es#p479\n",
      "allowed to fetch : http://www.capcom.co.jp/bio_series/biohazard2.html\n",
      "allowed to fetch : http://www.imdb.com/title/tt0161941/\n",
      "allowed to fetch : https://www.wikidata.org/wiki/q733992\n",
      "allowed to fetch : http://www.hobbyconsolas.com/reportajes/biohazard-resident-evil-juegos-con-varios-nombres-68074\n",
      "allowed to fetch : http://www.webcitation.org/5xtbyewcd\n",
      "allowed to fetch : http://www.squareenixmusic.com/features/interviews/shusakuuchiyama.shtml\n",
      "allowed to fetch : http://www.vgmonline.net/reviews/chris/biohazard2.shtml\n",
      "allowed to fetch : http://www.gamespot.com/reviews/resident-evil-2-review/1900-2549080/\n",
      "not allowed to fetch : http://web.archive.org/web/http://www.capcom.co.jp/newproducts/consumer/bio2/zapping.html\n",
      "allowed to fetch : http://www.capcom.co.jp/newproducts/consumer/bio2/zapping.html\n",
      "allowed to fetch : http://guidesarchive.ign.com/guides/10798/secrets.html\n",
      "allowed to fetch : http://www.webcitation.org/5wdw7jh6k\n",
      "allowed to fetch : http://www.allgame.com/game.php?id=28624&tab=review\n",
      "An exception ocurred : 'utf-8' codec can't decode byte 0xe9 in position 64972: invalid continuation byte\n",
      "http://www.vgchartz.com/gamedb/?name=resident+evil+2&publisher=&platform=&genre=&minsales=0&results=200\n",
      "\n",
      ". . .\n",
      "not allowed to fetch : http://www.vgchartz.com/gamedb/?name=resident+evil+2&publisher=&platform=&genre=&minsales=0&results=200\n",
      "allowed to fetch : http://www.capcom.co.jp/ir/english/business/million.html\n",
      "allowed to fetch : http://www.webcitation.org/5zvykojqb\n",
      "not allowed to fetch : http://www.eluniversal.com.mx/articulo/techbit/2015/08/12/confirman-remake-de-resident-evil-2\n",
      "allowed to fetch : http://www.ign.com/articles/2009/03/06/resident-evil-the-many-looks-of-the-tyrant?page=1\n",
      "not allowed to fetch : http://web.archive.org/web/20090219164142/http://www.gamepro.com/article/reviews/53571/resident-evil-2/\n",
      "allowed to fetch : http://www.gamepro.com/article/reviews/53571/resident-evil-2/\n",
      "allowed to fetch : http://guidesarchive.ign.com/guides/10402/characterstrategy.html\n",
      "allowed to fetch : http://www.webcitation.org/5wdtid8rq\n",
      "allowed to fetch : http://www.gamepro.com/article/reviews/751/resident-evil-2-platinum/\n",
      "allowed to fetch : http://guidesarchive.ign.com/guides/10798/rankings.html\n",
      "allowed to fetch : http://dreamcast.ign.com/articles/163/163373p1.html\n",
      "allowed to fetch : http://guidesarchive.ign.com/guides/10402/survivalguide.html\n",
      "not allowed to fetch : http://replay.web.archive.org/20050206023353/http://www.capcom.co.jp/studio4/index.html\n",
      "allowed to fetch : http://www.capcom.co.jp/studio4/index.html\n",
      "not allowed to fetch : http://web.archive.org/web/19970120062700/http://www.capcoment.com/buzzbin/pr/resevil2.html\n",
      "allowed to fetch : http://www.capcoment.com/buzzbin/pr/resevil2.html\n",
      "allowed to fetch : http://www.gamespot.com/videos/history-of-resident-evil/2300-6112074/\n",
      "allowed to fetch : http://guidesarchive.ign.com/guides/10798/refacts.html\n",
      "allowed to fetch : http://www.webcitation.org/5wduoxbsq\n",
      "allowed to fetch : http://okatuku.ciao.jp/diary/arc/bn2005_02.html\n",
      "allowed to fetch : http://ign64.ign.com/articles/160/160798p1.html\n",
      "allowed to fetch : http://www.vgmonline.net/reviews/chris/biohazard2complete.shtml\n",
      "allowed to fetch : http://www.webcitation.org/6hcps4exb\n",
      "allowed to fetch : http://www.capcom.co.jp/newproducts/consumer/bio/bio.html\n",
      "allowed to fetch : http://www.gamespot.com/reviews/resident-evil-2-review/1900-2532786/\n",
      "allowed to fetch : http://www.webcitation.org/6hcpt1qyt\n",
      "allowed to fetch : http://www.capcom.co.jp/newproducts/consumer/value/\n",
      "not allowed to fetch : http://www.sourcenext.com/products/capcom/bio2.html\n",
      "allowed to fetch : http://www.webcitation.org/5wduzxgv4\n",
      "allowed to fetch : http://game.watch.impress.co.jp/docs/20060124/bio.htm\n",
      "allowed to fetch : http://www.webcitation.org/5wduh6ylg\n",
      "allowed to fetch : http://www.gamespot.com/reviews/resident-evil-2-review/1900-2540112/\n",
      "not allowed to fetch : http://blog.us.playstation.com/2009/11/19/playstation-store-update-112/\n",
      "allowed to fetch : http://www.webcitation.org/5wduzequo\n",
      "not allowed to fetch : http://www.gamasutra.com/view/feature/3148/postmortem_angel_studios_.php\n",
      "not allowed to fetch : http://web.archive.org/web/20020609084201/http://www.angelstudios.com/games/re2/re2_features_add.html\n",
      "allowed to fetch : http://www.angelstudios.com/games/re2/re2_features_add.html\n",
      "not allowed to fetch : http://web.archive.org/web/20020609084622/http://www.angelstudios.com/games/re2/re2_features_enhance.html\n",
      "allowed to fetch : http://www.angelstudios.com/games/re2/re2_features_enhance.html\n",
      "allowed to fetch : http://ign64.ign.com/articles/068/068529p1.html\n",
      "allowed to fetch : http://www.webcitation.org/5wdunutnn\n",
      "allowed to fetch : http://www.allgame.com/game.php?id=38453&tab=review\n",
      "allowed to fetch : http://www.gamespot.com/news/2465273.html\n",
      "not allowed to fetch : http://web.archive.org/web/19991012050906/http://game.com/gamecom/re2.htm\n",
      "allowed to fetch : http://game.com/gamecom/re2.htm\n",
      "\n",
      ". . .\n",
      "step: 2\n",
      "batch size: 49\n",
      "crawled unique links: 50\n",
      "\n",
      "\n",
      "\n",
      ". . .\n",
      "step: 3\n",
      "batch size: 0\n",
      "crawled unique links: 50\n",
      "\n",
      "\n",
      "\n",
      ". . .\n",
      "step: 4\n",
      "batch size: 0\n",
      "crawled unique links: 50\n",
      "\n",
      "\n",
      "\n",
      ". . .\n",
      "step: 5\n",
      "batch size: 0\n",
      "crawled unique links: 50\n",
      "\n",
      "\n",
      "\n",
      ". . .\n",
      "FINISH!\n",
      "unique links size: 50\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import urllib.robotparser as robotparser\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "\n",
    "#import grequests\n",
    "\n",
    "\n",
    "#init:\n",
    "#initial_url_address = 'https://machinelearningmastery.com/compare-performance-machine-learning-algorithms-weka/'\n",
    "#initial_url_address = 'http://guidesarchive.ign.com/guides/10798/secrets.html'\n",
    "initial_url_address = 'https://es.wikipedia.org/wiki/Resident_Evil_2' \n",
    "current_batch = [initial_url_address] #initialize crawl batch\n",
    "crawled_links = {initial_url_address} #hold the overall url links repo\n",
    "max_depth = 5 #how deep is the breadth-first approach\n",
    "max_crawl_files = 500\n",
    "\n",
    "csv_file_name = \"crawlResultsCsv_\" + str(round(time.time())) + \".csv\" #csv unique name\n",
    "\n",
    "print(\"starting the crawler...\")\n",
    "print(\"saving results on %s\" %csv_file_name)\n",
    "print(\"remember to do the robot thing!!\")\n",
    "        \n",
    "\n",
    "#TODO: \n",
    "# 2. Make asyncronous\n",
    "\n",
    "    \n",
    "\n",
    "def getLinksFromPage(url):\n",
    "    page_links = [] #links in this page\n",
    "        \n",
    "    resp = requests.get(url)\n",
    "    html = BeautifulSoup(resp.text, 'html.parser') #get html\n",
    "    for a in html.find_all('a'):\n",
    "        link = a.get('href')\n",
    "        if isinstance(link, str) and link.startswith('http'):\n",
    "            link = link.lower()\n",
    "            appendRowToCsvFile(link, csv_file_name)\n",
    "            page_links.append(link) #extract href url\n",
    "    return page_links\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "# main loop\n",
    "for i in range(max_depth):\n",
    "    print(\"\")\n",
    "    print(\". . .\")\n",
    "    print(\"step: %s\" % (i+1))\n",
    "    print(\"batch size: %s\" % len(current_batch))\n",
    "    print(\"crawled unique links: %s\" % len(crawled_links))\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "\n",
    "    next_batch = []\n",
    "    \n",
    "    for url in current_batch:  \n",
    "        page_links = getLinksFromPage(url)\n",
    "        for link in page_links:\n",
    "            if (len(crawled_links) >= max_crawl_files):\n",
    "                break\n",
    "            if link not in crawled_links:\n",
    "                if canFetchUrl(link):\n",
    "                    #Here we only save the allowed links\n",
    "                    print(\"allowed to fetch : %s\" % link)\n",
    "                    crawled_links.add(link)\n",
    "                    appendRowToCsvFile(link, csv_file_name)\n",
    "                    next_batch.append(link) #since it was a new link, we will crawl it in the next batch                    \n",
    "                else: print(\"not allowed to fetch : %s\" % link)\n",
    "    current_batch = next_batch\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"\")\n",
    "print(\". . .\")\n",
    "print(\"FINISH!\")\n",
    "print(\"unique links size: %s\" % len(crawled_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434\n"
     ]
    }
   ],
   "source": [
    "# Optional, for loading the results CSV:\n",
    "test_set={initial_url_address}\n",
    "loadCsvToSet(test_set, csv_file_name)\n",
    "\n",
    "print(len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "..\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## test robots.txt\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "test1 = 'https://www.buzzfeed.com/buzzfeed/api/' #not allowed by robot.txt\n",
    "test2 = 'https://github.com/lluissuros?tab=overview&from=2019-02-01&to=2019-02-07' # not allowed?\n",
    "test3 = 'https://slate.com/culture/sports' #allowed\n",
    "test4 = 'http://www.capcom.co.jp/newproducts/consumer/value/' #no robots.txt, therefore allowed\n",
    "\n",
    "\n",
    "print(hasRobotText(test1))\n",
    "print(hasRobotText(test2))\n",
    "print(hasRobotText(test3))\n",
    "print(hasRobotText(test4))\n",
    "\n",
    "print(\"..\")\n",
    "\n",
    "print(canFetchUrl(test1))\n",
    "print(canFetchUrl(test2))\n",
    "print(canFetchUrl(test3))\n",
    "print(canFetchUrl(test4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the crawler...\n",
      "saving results on crawlResultsCsv_1549549953.csv\n"
     ]
    }
   ],
   "source": [
    "csv_file_name = \"crawlResultsCsv_\" + str(round(time.time())) + \".csv\" #csv unique name\n",
    "\n",
    "\n",
    "print(\"starting the crawler...\")\n",
    "print(\"saving results on %s\" %csv_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
