{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install beautifulsoup4\n",
    "!pip3 install lxml\n",
    "!pip3 install html5lib\n",
    "!pip3 install grequests\n",
    "!pip3 install gevent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv function definitions:\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "def saveSetToFile(my_set, csv_file_name):\n",
    "    for url in my_set:\n",
    "        appendRowToCsvFile(url, csv_file_name)\n",
    "        \n",
    "        \n",
    "def appendRowToCsvFile(csvRow, csv_file_name):\n",
    "    with open(csv_file_name, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([csvRow])\n",
    "\n",
    "        \n",
    "def loadCsvToSet(my_set, csv_file_name):\n",
    "    mycsv = csv.reader(open(csv_file_name))\n",
    "    for row in mycsv:\n",
    "        url = row[0]\n",
    "        my_set.add(url)\n",
    "\n",
    "        \n",
    "#saveSetToFile(crawled_links, csv_file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the crawler...\n",
      "remember to do the robot thing!!\n",
      "\n",
      ". . .\n",
      "step: 1\n",
      "batch size: 1\n",
      "crawled unique links: 1\n",
      "\n",
      "\n",
      "allowed to fetch : https://www.wikidata.org/wiki/q733992?uselang=es#p348\n",
      "allowed to fetch : https://www.wikidata.org/wiki/q733992?uselang=es#p479\n",
      "allowed to fetch : http://www.capcom.co.jp/bio_series/biohazard2.html\n",
      "allowed to fetch : http://www.imdb.com/title/tt0161941/\n",
      "\n",
      ". . .\n",
      "step: 2\n",
      "batch size: 4\n",
      "crawled unique links: 5\n",
      "\n",
      "\n",
      "\n",
      ". . .\n",
      "step: 3\n",
      "batch size: 0\n",
      "crawled unique links: 5\n",
      "\n",
      "\n",
      "\n",
      ". . .\n",
      "step: 4\n",
      "batch size: 0\n",
      "crawled unique links: 5\n",
      "\n",
      "\n",
      "\n",
      ". . .\n",
      "step: 5\n",
      "batch size: 0\n",
      "crawled unique links: 5\n",
      "\n",
      "\n",
      "\n",
      ". . .\n",
      "FINISH!\n",
      "unique links size: 5\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import urllib.robotparser as robotparser\n",
    "#import grequests\n",
    "\n",
    "\n",
    "#init:\n",
    "#initial_url_address = 'https://machinelearningmastery.com/compare-performance-machine-learning-algorithms-weka/'\n",
    "#initial_url_address = 'http://guidesarchive.ign.com/guides/10798/secrets.html'\n",
    "initial_url_address = 'https://es.wikipedia.org/wiki/Resident_Evil_2' \n",
    "current_batch = [initial_url_address] #initialize crawl batch\n",
    "crawled_links = {initial_url_address} #hold the overall url links repo\n",
    "max_depth = 5 #how deep is the breadth-first approach\n",
    "max_crawl_files = 5\n",
    "csv_file_name = \"crawlResultsCsv.csv\" #where you want the file to be saved to. At the moment, csv is in the same path\n",
    "\n",
    "\n",
    "print(\"starting the crawler...\")\n",
    "print(\"remember to do the robot thing!!\")\n",
    "        \n",
    "\n",
    "#TODO: \n",
    "# 1. Confirm that it agrees to be checked (robots.txt, crawling frequency) => NOT SURE\n",
    "# 2. Make asyncronous\n",
    "\n",
    "\n",
    "\n",
    "def canFetchUrl(url):\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    can_fetch = False\n",
    "    try:\n",
    "        rp.set_url(url) #robotparser get address\n",
    "        rp.read() #robotparser read  \n",
    "        can_fetch = rp.can_fetch(\"*\",url) # true if we can fetch the url according to robot.txt\n",
    "    except BaseException as e:\n",
    "        print(\"An exception ocurred : %s\" % str(e)) \n",
    "        print(url)\n",
    "        print(\"\")\n",
    "        print(\". . .\")     \n",
    "    return can_fetch\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def getLinksFromPage(url):\n",
    "    page_links = [] #links in this page\n",
    "        \n",
    "    resp = requests.get(url)\n",
    "    html = BeautifulSoup(resp.text, 'html.parser') #get html\n",
    "    for a in html.find_all('a'):\n",
    "        link = a.get('href')\n",
    "        if isinstance(link, str) and link.startswith('http'):\n",
    "            link = link.lower()\n",
    "            appendRowToCsvFile(link, csv_file_name)\n",
    "            page_links.append(link) #extract href url\n",
    "    return page_links\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "# main loop\n",
    "for i in range(max_depth):\n",
    "    print(\"\")\n",
    "    print(\". . .\")\n",
    "    print(\"step: %s\" % (i+1))\n",
    "    print(\"batch size: %s\" % len(current_batch))\n",
    "    print(\"crawled unique links: %s\" % len(crawled_links))\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "\n",
    "    next_batch = []\n",
    "    \n",
    "    for url in current_batch:  \n",
    "        page_links = getLinksFromPage(url)\n",
    "        for link in page_links:\n",
    "            if (len(crawled_links) >= max_crawl_files):\n",
    "                break\n",
    "            if link not in crawled_links:\n",
    "                if canFetchUrl(link):\n",
    "                    #Here we only save the allowed links\n",
    "                    print(\"allowed to fetch : %s\" % link)\n",
    "                    crawled_links.add(link)\n",
    "                    appendRowToCsvFile(link, csv_file_name)\n",
    "                    next_batch.append(link) #since it was a new link, we will crawl it in the next batch                    \n",
    "                else: print(\"not allowed to fetch : %s\" % link)\n",
    "    current_batch = next_batch\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"\")\n",
    "print(\". . .\")\n",
    "print(\"FINISH!\")\n",
    "print(\"unique links size: %s\" % len(crawled_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434\n"
     ]
    }
   ],
   "source": [
    "# Optional, for loading the results CSV:\n",
    "test_set={initial_url_address}\n",
    "loadCsvToSet(test_set, csv_file_name)\n",
    "\n",
    "print(len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
